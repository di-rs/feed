---
title: "Finding my Frontier: Cloud free coding on GLM-5"
date: 2026-02-18
time: "00:00"
source: DEV Community
link: https://dev.to/mfolsom/finding-my-frontier-cloud-free-coding-on-glm-5-47o4
image: ""
tags: [local-llm, glm-5, ai, m3-ultra, llama-cpp, open-source]
---

A hands-on experiment running the frontier-sized GLM-5 model locally on an M3 Ultra Mac Studio with 512GB RAM for agentic coding â€” initial MLX attempts yielded 30+ minute response times, while GGUF quantization via llama.cpp showed more promise. The article documents the trial-and-error of finding a viable cloud-free AI coding setup, revealing the real performance constraints when pushing consumer hardware to its limits with massive open-source models.
